{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Local kubernetes cluster","text":"<p>For testing pupose, you can use k3d:</p> <pre><code>k3d cluster create k8s-test \\\n  --servers 3 \\\n  --agents 3 \\\n  -p \"80:80@loadbalancer\" \\\n  -p \"443:443@loadbalancer\" \\\n  --k3s-node-label \"type=control@server:0,1,2\" \\\n  --k3s-node-label \"type=worker@agent:0,1,2\"\n</code></pre>"},{"location":"#nginx-controller","title":"Nginx controller","text":"<pre><code>k3d cluster create k8s-test \\\n  --servers 3 \\\n  --agents 3 \\\n  --k3s-arg \"--disable=traefik@server:*\" \\\n  --volume /mnt/ssd1tbp2/data/:/mnt/data/ \\\n  --k3s-node-label \"type=control@server:0,1,2\" \\\n  --k3s-node-label \"type=worker@agent:0,1,2\"\n</code></pre> <pre><code>helm repo add nginx-stable https://helm.nginx.com/stable\nhelm repo update\nhelm install nginx-ingress nginx-stable/nginx-ingress\n</code></pre>"},{"location":"#cluster-overview","title":"Cluster overview","text":""},{"location":"#control-plane-components","title":"Control Plane Components","text":"<p>The control plane's components make global decisions about the cluster (for example, scheduling), as well as detecting and responding to cluster events (for example, starting up a new pod when a Deployment's replicas field is unsatisfied). Control plane components can be run on any machine in the cluster. However, for simplicity, setup scripts typically start all control plane components on the same machine, and do not run user containers on this machine. See Creating Highly Available clusters with kubeadm for an example control plane setup that runs across multiple machines.</p>"},{"location":"#kube-apiserver","title":"kube-apiserver","text":"<p>The API server is a component of the Kubernetes control plane that exposes the Kubernetes API. The API server is the front end for the Kubernetes control plane.</p> <p>The main implementation of a Kubernetes API server is kube-apiserver. kube-apiserver is designed to scale horizontally\u2014that is, it scales by deploying more instances. You can run several instances of kube-apiserver and balance traffic between those instances.</p>"},{"location":"#etcd","title":"etcd","text":"<p>Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data. If your Kubernetes cluster uses etcd as its backing store, make sure you have a back up plan for the data. You can find in-depth information about etcd in the official documentation.</p>"},{"location":"#kube-scheduler","title":"kube-scheduler","text":"<p>Control plane component that watches for newly created Pods with no assigned node, and selects a node for them to run on. Factors taken into account for scheduling decisions include: individual and collective resource requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference, and deadlines.</p>"},{"location":"#kube-controller-manager","title":"kube-controller-manager","text":"<p>Control plane component that runs controller processes. Logically, each controller is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process. There are many different types of controllers. Some examples of them are:</p> <ul> <li>Node controller: Responsible for noticing and responding when nodes go down.</li> <li>Job controller: Watches for Job objects that represent one-off tasks, then creates Pods to run those tasks to completion.</li> <li>EndpointSlice controller: Populates EndpointSlice objects (to provide a link between Services and Pods).</li> <li>ServiceAccount controller: Create default ServiceAccounts for new namespaces.</li> </ul> <p>The above is not an exhaustive list.</p>"},{"location":"#cloud-controller-manager","title":"cloud-controller-manager","text":"<p>A Kubernetes control plane component that embeds cloud-specific control logic. The cloud controller manager lets you link your cluster into your cloud provider's API, and separates out the components that interact with that cloud platform from components that only interact with your cluster. The cloud-controller-manager only runs controllers that are specific to your cloud provider. If you are running Kubernetes on your own premises, or in a learning environment inside your own PC, the cluster does not have a cloud controller manager.</p> <p>As with the kube-controller-manager, the cloud-controller-manager combines several logically independent control loops into a single binary that you run as a single process. You can scale horizontally (run more than one copy) to improve performance or to help tolerate failures.</p> <p>The following controllers can have cloud provider dependencies:</p> <ul> <li>Node controller: For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding</li> <li>Route controller: For setting up routes in the underlying cloud infrastructure</li> <li>Service controller: For creating, updating and deleting cloud provider load balancers</li> </ul>"},{"location":"#node-components","title":"Node Components","text":"<p>Node components run on every node, maintaining running pods and providing the Kubernetes runtime environment.</p>"},{"location":"#kubelet","title":"kubelet","text":"<p>An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod. The kubelet takes a set of PodSpecs that are provided through various mechanisms and ensures that the containers described in those PodSpecs are running and healthy. The kubelet doesn't manage containers which were not created by Kubernetes.</p>"},{"location":"#kube-proxy","title":"kube-proxy","text":"<p>kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept. kube-proxy maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster. kube-proxy uses the operating system packet filtering layer if there is one and it's available. Otherwise, kube-proxy forwards the traffic itself.</p>"},{"location":"#container-runtime","title":"Container runtime","text":"<p>A fundamental component that empowers Kubernetes to run containers effectively. It is responsible for managing the execution and lifecycle of containers within the Kubernetes environment.</p> <p>Kubernetes supports container runtimes such as containerd, CRI-O, and any other implementation of the Kubernetes CRI (Container Runtime Interface).</p>"},{"location":"deployment/deployment/","title":"Deployment","text":"<p>The Deployment resource exists in the apps/v1 API and defines all supported attributes and capabilities.</p> <p>The Deployment controller runs on the control plane, watches Deployments, and reconciles observed state with desired state.</p> <p></p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-deploy\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: hello-world\n  revisionHistoryLimit: 5\n  progressDeadlineSeconds: 300\n  minReadySeconds: 10\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n      maxSurge: 1\n  template:\n    metadata:\n      labels:\n        app: hello-world\n    spec:\n      containers:\n      - name: hello-pod\n        image: nigelpoulton/k8sbook:1.0\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 0.1\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: hello-svc\n  labels:\n    app: hello-world\nspec:\n  ports:\n  - port: 80\n    targetPort: 8080\n    protocol: TCP\n  selector:\n    app: hello-world\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: hello-svc\n\nspec:\n  ingressClassName: traefik\n  rules:\n  - host: localhost \n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: hello-svc\n            port:\n              number: 80\n</code></pre>"},{"location":"deployment/scaling/","title":"Scaling","text":"<p>The Horizontal Pod Autoscaler (HPA) adds and removes Pods to meet current demand. Most clusters install it by default, and it\u2019s widely used.</p> <p>The Cluster Autoscaler (CA) adds and removes cluster nodes so you always have enough to run all scheduled Pods. This is also installed by default and widely used.</p> <p></p> <p>Multi-dimensional autoscaling: This is jargon for combining multiple scaling methods \u2014 scaling Pods and nodes, or scaling apps horizontally (adding more Pods) and vertically (adding more resources to existing Pods).</p>"},{"location":"deployment/sidecar-pod/","title":"Sidecar Pod","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: git-sync\n  labels:\n    app: sidecar\nspec:\n  containers:\n  - name: ctr-web\n    image: nginx\n    volumeMounts:\n    - name: html\n      mountPath: /usr/share/nginx/\n  - name: ctr-sync\n    image: k8s.gcr.io/git-sync:v3.1.6\n    volumeMounts:\n    - name: html\n      mountPath: /tmp/git\n    env:\n    - name: GIT_SYNC_REPO\n      value: https://github.com/nigelpoulton/ps-sidecar.git\n    - name: GIT_SYNC_BRANCH\n      value: master\n    - name: GIT_SYNC_DEPTH\n      value: \"1\"\n    - name: GIT_SYNC_DEST\n      value: \"html\"\n  volumes:\n  - name: html\n    emptyDir: {}\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: svc-sidecar\nspec:\n  selector:\n    app: sidecar\n  ports:\n  - port: 80\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: svc-sidecar\n  annotations:\n\nspec:\n  ingressClassName: traefik\n  rules:\n  - host: localhost \n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: svc-sidecar\n            port:\n              number: 80\n</code></pre>"},{"location":"ingress/config/","title":"Config","text":"<pre><code>kubectl get all -A\n\nkube-system  service/traefik  LoadBalancer 10.43.123.79  172.19.0.2,172.19.0.3,172.19.0.4,172.19.0.5,172.19.0.6,172.19.0.7   80:31183/TCP,443:31980/TCP   53s\n</code></pre>"},{"location":"ingress/config/#etchosts","title":"/etc/hosts","text":"<pre><code>172.19.0.2  cluster.xyz     \n172.19.0.2  hydra.cluster.xyz   \n172.19.0.2  shield.cluster.xyz  \n</code></pre>"},{"location":"ingress/config/#ingress","title":"Ingress","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ingress\n  annotations:\n    treafik.ingress.kubernetes.io/rewrite-target: /\nspec:\n  ingressClassName: traefik\n  rules:\n  - host: shield.cluster.xyz\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: svc-shield\n            port:\n              number: 8080\n  - host: hydra.cluster.xyz\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: svc-hydra\n            port:\n              number: 8080\n  - host: cluster.xyz\n    http:\n      paths:\n      - path: /shield\n        pathType: Prefix\n        backend:\n          service:\n            name: svc-shield\n            port:\n              number: 8080\n      - path: /hydra\n        pathType: Prefix\n        backend:\n          service:\n            name: svc-hydra\n            port:\n              number: 8080\n</code></pre>"},{"location":"ingress/ingress/","title":"Ingress","text":"<p>Ingress expose multiple Services through a single cloud load balancer. It does this by creating a single cloud load balancer on port 80 or 443 and using host- based and path-based routing to map connections to different Services on the cluster.</p> <p></p>"},{"location":"kubectl/commands/","title":"Commands","text":""},{"location":"kubectl/commands/#kube-context","title":"kube context","text":"<pre><code>kubectl config view\nkubectl config current-context\nkubectl config use-context foo\n</code></pre>"},{"location":"kubectl/commands/#ns-context","title":"ns context","text":"<pre><code>kubectl config set-context --current --namespace bar\n</code></pre>"},{"location":"kubectl/commands/#rollouts","title":"Rollouts","text":"<pre><code>kubectl rollout pause deploy &lt;m-name&gt;\nkubectl rollout resume deploy &lt;m-name&gt;\n</code></pre> <pre><code>kubectl rollout history deployment &lt;m-name&gt;\nkubectl rollout undo deployment &lt;m-name&gt; --to-revision=1\n</code></pre>"},{"location":"kubectl/commands/#services","title":"Services","text":"<pre><code>kubectl expose deployment svc-test --type=LoadBalancer\nkubectl get endpointslices\nkubectl describe endpointslice &lt;slice-name&gt;\n</code></pre>"},{"location":"kubectl/config/","title":"Config","text":"<pre><code>source &lt;(kubectl completion bash)\n</code></pre> <p>Add the following to your <code>.bashrc</code> file:</p> <pre><code>alias k='kubectl'\ncomplete -F __start_kubectl k\n</code></pre> <pre><code>source ~/.bashrc\n</code></pre>"},{"location":"services/service-discovery/","title":"Service Discovery","text":"<p>Apps need two things to be able to send requests to other apps: 1. A way to know the name of the other app (the name of its Service) 2. A way to convert the name into an IP address</p> <p></p> <p>Every Kubernetes cluster has a built-in cluster DNS that it uses as its service registry. It\u2019s a Kubernetes-native application running on the control plane of every Kubernetes cluster as two or more Pods managed by a Deployment and fronted by a Service. The Deployment is usually called coredns or kube-dns, and the Service is always called kube-dns.</p> <p></p> <pre><code>kubectl get pods -n kube-system -l k8s-app=kube-dns\nkubectl get deploy -n kube-system -l k8s-app=kube-dns\nkubectl get svc -n kube-system -l k8s-app=kube-dns\n</code></pre> <p></p> <p>You post a new Service resource manifest to the API server, where it\u2019s authenticated and authorised. Kubernetes allocates it a ClusterIP and persists its configuration to the cluster store. The cluster DNS observes the new Service and registers the appropriate DNS A and SRV records. Associated EndpointSlice objects are created to hold the list of healthy Pod IPs that match the Service\u2019s label selector. Every node runs a kube-proxy that observes the new objects and creates local routing rules so that requests to the Service\u2019s ClusterIP get routed to Pods.</p> <p></p>"},{"location":"services/service-discovery/#debug","title":"Debug","text":"<p>A common way to test if the cluster DNS is working is to use nslookup to resolve the kubernetes Service.</p> <pre><code>kubectl run -it dnsutils \\\n--image registry.k8s.io/e2e-test-images/jessie-dnsutils:1.7\n</code></pre>"},{"location":"services/service-types/","title":"Types","text":"<p>Kubernetes has several types of Services for different use cases and requirements. The major ones are:</p> <ul> <li>ClusterIP</li> <li>NodePort</li> <li>LoadBalance</li> </ul> <p></p> <p></p> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: svc-test\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      chapter: services\n  template:\n    metadata:\n      labels:\n        chapter: services\n    spec:\n      containers:\n      - name: hello-ctr\n        image: nigelpoulton/k8sbook:1.0\n        ports:\n        - containerPort: 8080\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: cloud-lb\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 9000\n    targetPort: 8080\n  selector:\n    chapter: services\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: svc-test\n  labels:\n    chapter: services\nspec:\n  ipFamilyPolicy: PreferDualStack\n  ipFamilies:\n    - IPv4\n    - IPv6\n  type: NodePort\n  ports:\n    - port: 8080\n      nodePort: 30001\n      targetPort: 9090\n      protocol: TCP\n  selector:\n    chapter: services\n</code></pre>"},{"location":"services/services/","title":"Labels","text":"<p>Services use labels and selectors to know which Pods to send traffic to. This is the same technology that loosely couples Deployments to Pods.</p> <p></p> <p>You create a Service, and the EndpointSlice controller automatically creates an associated EndpointSlice object. Kubernetes then watches the cluster, looking for Pods matching the Service\u2019s label selector. Any new Pods matching the selector are added to the EndpointSlice, whereas any deleted Pods get removed. Applications send traffic to the Service name, and the application\u2019s container uses the cluster DNS to resolve the name to an IP address. The container then sends the traffic to the Service\u2019s IP, and the Service forwards it to one of the Pods listed in the EndpointSlice.</p>"},{"location":"storage/csi/","title":"CSI","text":"<p>Container Storage Interface is an open-source project that defines an industry-standard interface  so container orchestrators can leverage external storage resources in a uniform way.  For example, it gives storage providers a documented interface to work with. It also means that CSI plugins should work on any orchestration platform that supports the CSI.</p> <p>You can find a relatively up-to-date list of CSI plugins in the following repository. The repository refers to plugins as drivers.</p> <pre><code>https://kubernetes-csi.github.io/docs/drivers.html\n</code></pre> <p></p>"},{"location":"storage/pvs/","title":"PV Subsystem","text":"<p>Persistent Volume Subsystem is a standardized set of API objects that make it easy for applications running on Kubernetes to consume storage. There are a growing number of storage-related API objects, but the core ones are:</p> <ul> <li>PersistentVolumes (PV)</li> <li>PersistentVolumeClaims (PVC)</li> <li>StorageClasses (SC)</li> </ul> <p></p> <ol> <li>The Pod needs a volume and requests it via a PersistentVolumeClaim</li> <li>The PVC asks the StorageClass to create a new PV and associated volume on the AWS backend</li> <li>The SC makes the call to the AWS backend via the CSI plugin</li> <li>The CSI plugin creates the device (50GB EBS volume) on AWS</li> <li>The CSI plugin reports the creation of the external volume back to the SC</li> <li>The SC creates the PV and maps it to the EBS volume on the AWS back end</li> <li>The Pod mounts the PV and uses it</li> </ol>"},{"location":"storage/redis-cluster/","title":"Redis Cluster","text":"<p>Check the storageclass for host path provisioner</p> <pre><code>kubectl get sc\n</code></pre>"},{"location":"storage/redis-cluster/#deploy-our-statefulset","title":"Deploy our statefulset","text":"<pre><code>kubectl apply -f .\\kubernetes\\statefulsets\\statefulset.yaml\n</code></pre>"},{"location":"storage/redis-cluster/#enable-redis-cluster","title":"Enable Redis Cluster","text":"<pre><code>IPs=\"\"\n\nfor ip in $(kubectl get pods -l app=redis-cluster -o jsonpath='{range.items[*]}{.status.podIP} '); do\n  IPs+=\"$ip:6379 \"\ndone\n\nIPs=$(echo $IPs | sed 's/ $//')\n\nexport IPs\n</code></pre> <pre><code>kubectl exec -it redis-cluster-0  -- /bin/sh -c \"redis-cli -h 127.0.0.1 -p 6379 --cluster create ${IPs}\"\nkubectl exec -it redis-cluster-0  -- /bin/sh -c \"redis-cli -h 127.0.0.1 -p 6379 cluster info\"\n</code></pre> <pre><code>kubectl apply -f .\\kubernetes\\statefulsets\\example-app.yaml\n</code></pre> <pre><code>kubectl get pv\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                          STORAGECLASS   REASON   AGE\npvc-2166effb-2e0f-4b56-a9df-49a9e03a76b6   50Mi       RWO            Delete           Bound    default/data-redis-cluster-3   local-path              3m20s\npvc-2b9a9c6d-be8d-4ba1-805a-7706c64bd052   50Mi       RWO            Delete           Bound    default/data-redis-cluster-1   local-path              3m50s\npvc-45b051c7-947c-4c85-8a9c-16c34b15421d   50Mi       RWO            Delete           Bound    default/data-redis-cluster-0   local-path              4m6s\npvc-5653d345-1325-468f-bf9f-c1361b6bcb6a   50Mi       RWO            Delete           Bound    default/data-redis-cluster-2   local-path              3m35s\npvc-8a64f910-feab-4dd0-9bc2-e08a82f7e830   50Mi       RWO            Delete           Bound    default/data-redis-cluster-4   local-path              3m4s\npvc-a024f988-9326-4b54-b7ad-ff3726503a3c   50Mi       RWO            Delete           Bound    default/data-redis-cluster-5   local-path              2m48s\n\nkubectl get pvc\nNAME                   STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\ndata-redis-cluster-0   Bound    pvc-45b051c7-947c-4c85-8a9c-16c34b15421d   50Mi       RWO            local-path     4m15s\ndata-redis-cluster-1   Bound    pvc-2b9a9c6d-be8d-4ba1-805a-7706c64bd052   50Mi       RWO            local-path     3m59s\ndata-redis-cluster-2   Bound    pvc-5653d345-1325-468f-bf9f-c1361b6bcb6a   50Mi       RWO            local-path     3m44s\ndata-redis-cluster-3   Bound    pvc-2166effb-2e0f-4b56-a9df-49a9e03a76b6   50Mi       RWO            local-path     3m29s\ndata-redis-cluster-4   Bound    pvc-8a64f910-feab-4dd0-9bc2-e08a82f7e830   50Mi       RWO            local-path     3m14s\ndata-redis-cluster-5   Bound    pvc-a024f988-9326-4b54-b7ad-ff3726503a3c   50Mi       RWO            local-path     2m57s\n</code></pre>"},{"location":"storage/redis-cluster/#more-info","title":"More info","text":"<p>https://rancher.com/blog/2019/deploying-redis-cluster</p>"},{"location":"storage/sc/","title":"SC","text":"<p>StorageClasses let you define different classes of storage that apps can request. How you define your StorageClasses is up to you and will depend on the types of storage you have available.</p> <p>Main workflow:</p> <ol> <li>A normal Pod object</li> <li>The Pod requests a volume via the mypvc PVC</li> <li>The file defines a PVC called mypvc</li> <li>The PVC provisions a 50Gi volume</li> <li>The volume will be provisioned from the fast StorageClass</li> <li>The file defines the fast StorageClass</li> <li>The StorageClass provisions volumes via the pd.csi.storage.gke.io CSI plugin</li> <li>The CSI plugin will provision fast (pd-ssd) storage from the Google Cloud\u2019s storage backend</li> </ol> <pre><code>apiVersion: v1\nkind: Pod # &lt;&lt;==== 1. Pod\nmetadata:\n  name: mypod\nspec:\n  volumes:\n    - name: data\n      persistentVolumeClaim:\n        claimName: mypvc # &lt;&lt;==== 2. Request volume via the \"mypvc\" PVC\n  containers: ...\n  &lt;SNIP&gt;\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mypvc # &lt;&lt;==== 3. This is the \"mypvc\" PVC\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 50Gi # &lt;&lt;==== 4. Provision a 50Gi volume...\n  storageClassName: fast # &lt;&lt;==== 5. ...based on the \"fast\" StorageClass\n---\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: fast # &lt;&lt;==== 6. This is the \"fast\" StorageClass\nprovisioner: pd.csi.storage.gke.io # &lt;&lt;==== 7. Use this CSI plugin\nparameters:\n  type: pd-ssd # &lt;&lt;==== 8. Provision this type of storage\n</code></pre> <p>Kubernetes supports three volume access modes:</p> <ul> <li>ReadWriteOnce (RWO)</li> <li>ReadWriteMany (RWM)</li> <li>ReadOnlyMany (ROM)</li> </ul> <p>ReadWriteOnce lets a single PVC bind to a volume in read-write (R/W) mode. Attempts to bind it from multiple PVCs will fail.</p> <p>ReadWriteMany lets multiple PVCs bind to a volume in read-write (R/W) mode. File and object storage usually support this mode, whereas block storage usually doesn\u2019t.</p> <p>ReadOnlyMany allows multiple PVCs to bind to a volume in read-only (R/O) mode. It\u2019s also important to know that a PV can only be opened in one mode. For example, you cannot bind a single PV to one PVC in ROM mode and another PVC in RWM mode.</p>"}]}